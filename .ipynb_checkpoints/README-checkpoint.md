# ACL Anonymous Submission - Dialect Preference Bias in Large Language Models

## Project Overview

This project investigates Dialect Preference Bias in large language models (LLMs), with a focus on the contrast between Standard American English (SAE) and African American English (AAE). We define dialect preference bias as a systematic tendency for models to favor one dialect over another despite equivalent semantic content. Such bias can manifest in both model judgments and generation behavior, disadvantaging speakers of linguistically valid but socially marginalized dialects.

Beyond measuring dialect preference, this work aims to understand where such bias arises and how it can be mitigated. We combine decision-level and probability-level analyses to characterize dialect preference in LLMs, and we propose a lightweight, test-time dialect-sensitive steering method that intervenes in model representations without updating model weights. By steering inference toward less biased internal representations, our approach reduces dialect preference while preserving the base modelâ€™s general language capabilities.

## Method

### Activate the virtual environment
`source dialectic/bin/activate`

### Make sure you have required dependencies
`pip install -r requirements.txt`

### SAE-guided Semantic Segmentation
To get segmented dataset, run:
`python cleaned_dataset/segmentation.py --input ${data file location} \
    --output ${output location}`

## Key Findings

### Dialectic Group Invariance (DGI) Metrics

The DGI metric quantifies consistency in sentiment classification across dialects. Higher values indicate less bias.

*Running `obtain_sentiment_scripts/get_aae_sentiment.py` to get the sentiment of the original AAE sentences*

*Running `obtain_sentiment_scripts/get_sae_sentiment.py` to get the sentiment of the translated SAE sentences*

*Running `obtain_sentiment_scripts/get_aae_sentiment.py` to get the sentiment of the translated-back sentences*

### Perlexity Analysis

For vllm models:

`python perplexity/vllm_model_perplexity.py --data_file ${data file location} \
    --model ${model to run} \
    --output_dir ${output folder location} `

For huggingface source models:

`python perplexity/hf_model_perplexity.py --data_file ${data file location} \
    --model ${model to run} \
    --output_dir ${output folder location} `

### Preference Multiple Choice

`python preference_multiple_choice/pmc_task.py --input ${data file location} \
    --output ${output location} \
    --model ${model to run}`

Then run this to get anlyzed result:

`python preference_multiple_choice/analyze.py --input ${result generated by the previous step}`


### Feature-level Analysis

For vllm models:

`python log_difference/vllm_calculate_log_diff.py --input ${data file location} \
    --model ${model to run} \
    --output_dir ${output folder location} `

For huggingface source models:

`python log_difference/calculate_log_diff.py --input ${data file location} \
    --model ${model to run} \
    --output_dir ${output folder location} `

To get feature detected results, run:
`python log_difference/feture_detection.py --input ${data file location} \
    --output_dir ${output folder location} `


### Bias Mitigation

To get dialect vector run:
`python dialect-vector/calculate.py --model ${model to run} \
    --input ${segmented data} \
    --output ${output location}`

Performance Task Results:
`python dialect-vector/ppl_sweep.py --vector_pt ${pt file location} \
    --input_csv ${segmented data} \
    --output_csv ${output location} \
    --betas ${list of value to sweep} \
    --layer ${layer id to steer}`

Behavior Shift Results:
`python dialect-vector/behavior_shift.py --vector_pt ${pt file location} \
    --input_csv ${segmented data} \
    --output_csv ${output location} \
    --betas ${list of value to sweep} \
    --layer ${layer id to steer}`

Activation Shift Results:
`python dialect-vector/activation_shift.py --vector_pt ${pt file location} \
    --input_csv ${segmented data} \
    --output_csv ${output location} \
    --betas ${list of value to sweep} \
    --layer ${layer id to steer}`


## Conclusion

We presented a comprehensive study of dialect preference bias in large language models, focusing on African American English (AAE). Our work makes four contributions. First, we constructed and publicly released a corpus of over 16,000 AAE-SAE sentence pairs derived from authentic AAE tweets, providing the first large-scale resource for dialect bias evaluation grounded in naturally occurring AAE rather than synthetic transformations. Second, we demonstrated that state-of-the-art LLMs consistently favor SAE over AAE, both in discriminative settings (assigning inconsistent sentiment labels) and generative settings (preferring SAE continuations even under AAE context). Third, through feature-level analysis, we identified the linguistic triggers of this bias: preverbal markers such as finna and done act as universal bias triggers across all tested models, while syntactic features show model-specific effects. Finally, we showed that dialect-sensitive activation steering can mitigate this bias at inference time without retraining, offering a practical tool for fairer language model deployment.

Our findings have implications beyond AAE. The methodology we develop (constructing parallel corpora from authentic dialect sources, measuring bias through intrinsic language modeling metrics, and identifying feature-level triggers) can be extended to other marginalized language varieties. As LLMs become increasingly embedded in high-stakes applications, ensuring equitable treatment of dialectal variation is essential for building language technologies that serve all users.
